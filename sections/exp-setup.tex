\section{Experiment Environment}
\label{sec:setup}

We use four supercomputers as the computing environments for our performance study, naming the GADI system at the National Computational Infrastructure(NCI) in Australia, the ASPIRE-2A system at the National Supercomputing Centre (NSCC) in Singapore, and the Taiwania-2 and Taiwania-3 systems at the National Center for High-performance Computing (NCHC) in Taiwan. GADI and ASPIRE-2A are the designated supercomputers for the competition. Taiwania-2 and Taiwania-3 are the additional supercomputers we used for conducting more experiment runs and collecting more in-depth performance measurement results for tuning and analysis.  

%The hardware specifications and software configurations for all four systems are detailed below.

%\subsection{Hardware specification}

Table~\ref{table:hardware} summarizes the hardware specifications of the computing systems. It is clear that ASPIRE-2A has the strongest computing power equipped with 4 A100 GPUs and 64 cores per node. On the other hand, GADI has the highest interconnect network bandwidth at 200 Gb/s, while the other systems only have 100 GB/s network bandwidth. Taiwania-2 has a old CPU mode, but it has the most number of GPUs per node at 8. Taiwania-3 has no GPU, so we can only run MPAS on it, while Bloom was run on GADI, ASPIRE-2A and Taiwania-3. By conducting experiments across multiple systems, we are able to verify our experiment results in different computing environments, and observe the performance impact from hardware specifications and resource configurations. 


%Our testing environment comprises the GADI system at the National Computational Infrastructure(NCI) in Australia, the ASPIRE 2A system  at  the National Supercomputing Centre (NSCC) in Singapore and the Taiwania 3 system at the National Center for High-performance Computing (NCHC) in Taiwan . In the context of the MPAS-Atmosphere application, we utilized the normal queue on GADI and Taiwania 3. Additionally, on the other hand, for Bloom application, we employed the gpuvolta queue on GADI and Taiwania 2, while opting for the normal queue on the ASPIRE 2A system.

\begin{table*}[t]
    \centering
    \caption{The hardware configuration of the supercomputers used in our performance study.}
    \begin{tabular}{l|llll}
        \toprule
        Type & GADI & ASPIRE-2A  & Taiwania-2 & Taiwania-3\\
        \midrule
        CPU model & Intel Xeon Platinum 8274 & CPU AMD EPYC 7713  & Intel Xeon Gold 6154 & Intel Xeon Platinum 8280\\
        CPU architecture & Cascade Lake & Millan & Skylake &  Cascade Lake \\
        CPU frequency & 3.2 GHz & 2 GHz & 3.0 GHz & 2.4 GHz\\
        Cores per CPU & 24 & 32 & 18 & 28\\
        Numbers of CPU per node & 2 & 2 & 2 & 2\\
        RAM per node & 192GB & 512 GB & 192GB &  192GB   \\
        GPUs per node &  4 $\times$ NVIDIA V100 & 4 $\times$ NVIDIA A100 & 8 $\times$ NVIDIA V100 & Not Available  \\
        Network    & 200 Gb/s & 100 Gb/s & 100 Gb/s  & 100 Gb/s\\
        \midrule
        Operating System & Rocky Linux & RHEL8 & CentOS & CentOS \\
        Tested Application & Bloom, MPAS & Bloom & Bloom & MPAS \\
        \bottomrule
    \end{tabular}
    \label{table:hardware}
\end{table*}

%Table~\ref{table:hardware} summarizes some detailed hardware configurations. Our final result for MPAS-A consists of 32 CPU nodes with 48 CPU cores on each nodes, for Bloom inference, we utilized a total of 8 V100 GPUs distributed across two nodes on both GADI and Taiwania 2. Additionally,8 A100 GPUs were employed on two nodes on NSCC. . 


%\subsection{Software Configuration}
For the system environment, we installed the same software on different supercomputers for each application. According to the competition rule, we are allowed to select for the choice of MPAS version among the four available versions 5, 6, 7, and 8. We opted for the seventh and eighth version, which stand out as the latest version and most stable iteration matches the benchmark data among the four, and we could choose the version of supporting libraries and compilers for optimizing the application performance as well. On the other hand, there are three versions of the Bloom models. The first is the original BLOOM-176B model trained by Bigscience. Then, there are two models quantized by the Microsoft research team: bloom-deepspeed-inference-int8 and bloom-deepspeed-inference-fp16. Ultimately, we decided to use bloom-deepspeed-inference-int8 over the others because it offers the best combination of performance and feasibility on the Gadi supercomputer.


Table~\ref{table:software-CPU} and Table~\ref{table:software-GPU} lists the required software installed for achieving the best performance for MPAS and Bloom, respectively. But, we also installed other versions of compilers, MPI libraries and acceleration packages for performance comparison. 

\begin{table}[t]
    \centering
    \caption{The software installed for MPAS.}
    \begin{tabular}{ll}
        \toprule
        Libraries & Versions  \\
        \midrule
        C Compiler & ICC 2021.10.0 \\
        MPI Library & OpenMPI 4.1.5 \\
        HDF5 & 1.8.18 \\
        PNETCDF & 1.12.3 \\
        NETCDF C & 4.8.1 \\
        NETCDF Fortran & 4.6.1 \\
        Parallel I/O & 2.5.10 \\
        Metis & 5.1.0 \\
        \bottomrule
    \end{tabular}
    \label{table:software-CPU}
\end{table}

%Table~\ref{table:software-GPU} lists the software required from Bloom and installed on both the GADI ,Aspire2A and Taiwania 2 cluster. Furthermore, ASPIRE 2A use Red Hat Enterprise Linux 8 (RHEL) as operating system and Taiwania 2 use CentOS as operating system.

\begin{table}[th]
    \centering
    \caption{The software installed for Bloom.}
    \begin{tabular}{ll}
        \toprule
        Libraries & Versions  \\
        \midrule
        C Compiler & GCC 10.3.0 \\
        GPU Compiler & CUDA nvcc 11.6 \\
        python & 3.8 \\
        NCCL & 2.18 \\
        Pytorch & 1.12.1 \\
        Transformer & 4.26.1 \\
        Deepspeed & 0.7.6 \\
        accelerate & 0.16.0  \\
        gunicorn & 20.1.0 \\
        uvicorn & 0.19.0 \\
        jinja2 & 3.1.2 \\
        pydantic & 1.10.2 \\
        huggingface hub & 0.12.1 \\
        grpcio-tools & 1.50.0 \\
        bitsandbytes & 0.41.1 \\

        \bottomrule
    \end{tabular}
    \label{table:software-GPU}
\end{table}