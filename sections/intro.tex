\section{Introduction}
\label{sec:intro}
%\IEEEPARstart{A}{s} a special activity of the Student Cluster Competition at SC22 conference, we made an attempt to reproduce the performance evaluations of the \textit{Data-Centric (DaCe) Python framework} \cite{dace2021}. Ihis work aims to reproduce the results reported in \cite{dace2021} on a 4-node cluster equipped with AMD Zen 3 processors \cite{amdCPU} and NVIDIA A100 \cite{nvA100}.
% \IEEEPARstart{M}any frameworks are been proposed to support high-performance computing (HPC) with Python. For instance, Numba \cite{lam2015numba} is a just-in-time (JIT) compiler which translates a portion of Python code to pre-compiled machine code; NumPy features utilizing SIMD instructions and BLAS libraries. In addition, MPI4PY \cite{mpi4py} makes writing Python code with message passing interface (MPI) possible to support distributed computing. 


% \IEEEPARstart{P}ython has become one of the most used language \cite{dace2021}. Benefited from
% the NumPy \cite{harris2020array} library, many frameworks based on NumPy has developed, such as SciPY, Matplotlib, scikit and pandas. These frameworks make writing Python code with ease, and therefore are highly employed by both developers and users for scientific simulation \cite{2016md_simulation, ziogas2019quantum} and machine learning\cite{tensorflow2015-whitepaper,2019pytorch} domains.  As a result, several optimization libraries have been proposed to support high-performance computing (HPC) with Python. For instance, Numba \cite{lam2015numba} is a just-in-time (JIT) compiler which translates a portion of Python code to pre-compiled machine code; NumPy \cite{harris2020array} features utilizing SIMD instructions and BLAS libraries. In addition, MPI4PY \cite{mpi4py} makes writing Python code with message passing interface (MPI) possible to support distributed computing. 
\IEEEPARstart{P}ython has become one of the most widely used languages~\cite{dace2021} for scientific code. Its popularity is attributed to the availability of numerous frameworks based on NumPy~\cite{harris2020array}, such as SciPy~\cite{2020SciPy-NMeth}, scikit-learn~\cite{scikit-learn}, and pandas~\cite{mckinney-proc-scipy-2010}, which make writing Python code easier. These frameworks are extensively employed by developers and users in scientific simulation~\cite{2016md_simulation, ziogas2019quantum} and machine learning~\cite{tensorflow2015-whitepaper,2019pytorch} domains. However, existing Python libraries for high-performance computing (HPC) lack simultaneous support for the three Ps~\cite{20203P,dace2021}: Productivity, Portability, and Performance.
 
% Nonetheless, none of the existing libraries supports the three Ps \cite{20203P} (Productivity, Portability, Performance) on Python at the same time. Therefore, the Data-Centric (DaCe) Python \cite{dace2021} is proposed to bridge the gap. First, by using some annotating statements, users could easily employ DaCe to the existing program (Productivity). Second, DaCe provides several computing backend compatibilities such as CPU, GPU, FPGA and distributed computing (Portability). Last, DaCe could provide significant performance improvement compared the state-of-the-art libraries (Performance).
To bridge this gap, the Data-Centric (DaCe) Python framework~\cite{dace2021} is proposed. DaCe offers several advantages. Firstly, it allows users to integrate it into existing programs using annotating statements, enhancing productivity. Secondly, DaCe provides compatibility with various computing backends, including CPUs, GPUs, FPGAs, and distributed computing, ensuring portability. Lastly, DaCe demonstrates significant performance improvements compared to state-of-the-art libraries. DaCe identifies data parallelism and data reusing patterns in dataflows, optimizing them and converting Python functions to C code. The resulting code can be compiled to run efficiently on CPUs, GPUs, or FPGAs.

In this work, as part of the Student Cluster Competition (SCC) at the SC22 conference, we aim to reproduce the performance evaluations of DaCe on CPUs, GPUs, and multi-node supercomputers. Our cluster includes state-of-the-art CPUs and GPUs. Despite running on different architectures (Intel Xeon 6130 CPU and NVIDIA V100 GPU in \cite{dace2021}), our results successfully reproduce the evaluation work presented by the authors~\cite{dace2021}. The contributions of this paper are as follows: 
\circled{1}~We reproduced the CPU experiments on NumPy and DaCe.
\circled{2}~We reproduced the DaCe GPU experiments against NumPy.
\circled{3}~We reproduced the distributed experiments with the scale of at most eight nodes.
\circled{4}~We provide a comprehensive analysis of the benchmarks to explain how DaCe achieves performance enhancements.

The remainder of the paper is organized as follows: Section~\ref{sec:exp-setup} presents the hardware and software configurations used in the experiments. Section~\ref{sec:exp-run} describes the experimental procedures. Sections~\ref{sec:cpu-result}, \ref{sec:gpu-result}, and \ref{sec:distributed-result} evaluate the experiments. Finally, Section~\ref{sec:conc} concludes the paper.